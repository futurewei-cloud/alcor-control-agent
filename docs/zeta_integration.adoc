= Zeta Integration Design

== Architecture Overview

image::images/zeta_integration_high_level.jpg["highlevel Architecture", width=800, link="zeta_integration_high_level.jpg"]

1. Control/API Network. Alcor server configures the group entries and the buckets in the group entries for each computing node. 
Each group entry corresponds to a Zeta Gateway Cluster (ZGC). Each bucket in the group entry corresponds to a gateway in ZGC.

2. Tenant Network. The gateway node in ZGC is responsible for delivering the rules to ACA on computing nodes when two VMs communicate for the first time. 
ACA as the openflow controller will install rules on ovs. The path between the gateway node and the computing node is called the default path in the following.

== Bucketing System Design on Compute Node

The information of the ZGC cluster will be stored on the computing node. When a VM brings up, 
if the compute node does not have a VM with the same VNI, there is no need to configure flow table and group table.
The packet will match the VNI in the flow table to determine which group rule is used by this packet.
Each group rule can be configured with a few buckets, and each bucket specifies a gateway node.
After determining which group rule can handle the packet, the group rules support hash function to map a packet to a bucket. 
Ths hash field cna be specified as source MAC, source IP, destination MAC and destination IP.

image::images/zeta_integration_example.jpg["example", width=800, link="zeta_integration_example.jpg"]

We use an example to illustrate the default path. VMX belongs to VPC1, VMA belongs to VPC2. The flows of VPC1 and VPC2 are mapped to the same cluster, which includes two gateway nodes gateway1 and gateway2. The gw_port1 and gw_port2 ports of computing node1 are respectively connected to the gateway nodes gateway1 and gateway2. Then we deploy a group entry on computing node1, the group identifier is 4, and the action buckets are: bucket1:output=gw_port1; bucket2:output=gw_port2. In addition, by deploying a low-priority flow entry, the flow of the VPC1 that does not match the high-priority flow entry (vm-vm direct path forwarding entry) is directed to the group entry 4, so that all nodes on node1 are calculated. The flow of VPC1 will be directed to group entry 4, and will be forwarded to the gateway node gateway1 or gateway2 through the group entry 4. Through the flow table + group table, you can achieve the mapping of vpc to the gateway node cluster.


== Workflow Diagram for Default Path Configuration

Default path configuration happens when a VM brings up and there does not exist a VM which has the same VNI.

1. Alcor controller send a message to ACA on the host;

2. ACA receieves the message and extract information of ZGC from the database;

3. ACA configure ovs to install flow rule and group rule;

TBD

== Workflow Diagram for VM to VM Direct Path Configuration

This happens when a gateway node publishes rules to the computing node.

1. A gateway node receieve the first packet from a compute node;

2. The gateway node extract the header information of this packet;

3. The gateway node send the OAM packet to the computing node;

4. ACA on the compute node receieve the OAM packet;

5. ACA configure the flow table of ovs to determine which port go to the destination VM.

== OAM Packet Usage and Handling

TBD

== Workflow Diagram for Bucketing and OAM Handling.

TBD




== Code changes

=== Goal State Message Change from Alcor Server to ACA

TBD

=== OAM Packet Handling

TBD

=== Test Code Updates

TBD


